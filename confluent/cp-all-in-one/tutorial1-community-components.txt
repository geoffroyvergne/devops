https://docs.confluent.io/current/quickstart/cos-docker-quickstart.html#cos-docker-quickstart

https://docs.confluent.io/current/ksql/docs/tutorials/basics-docker.html

Create Kafka Topics :

users :
docker-compose exec broker kafka-topics --create --zookeeper \
zookeeper:2181 --replication-factor 1 --partitions 1 --topic users

print 'users';

pageviews :
docker-compose exec broker kafka-topics --create --zookeeper \
zookeeper:2181 --replication-factor 1 --partitions 1 --topic pageviews

print 'pageviews';


ksql-datagen pageviews :
wget https://github.com/confluentinc/kafka-connect-datagen/raw/master/config/connector_pageviews_cos.config
curl -X POST -H "Content-Type: application/json" --data @connector_pageviews_cos.config http://localhost:8083/connectors

ksql-datagen users :
wget https://github.com/confluentinc/kafka-connect-datagen/raw/master/config/connector_users_cos.config
curl -X POST -H "Content-Type: application/json" --data @connector_users_cos.config http://localhost:8083/connectors

Create and Write to a Stream and Table using KSQL :

Create Streams and Tables

Create a stream pageviews from the Kafka topic pageviews, specifying the value_format of AVRO.

CREATE STREAM pageviews \
  (viewtime BIGINT, userid VARCHAR, pageid VARCHAR) \
  WITH (KAFKA_TOPIC='pageviews', VALUE_FORMAT='AVRO');

Create a table users with several columns from the Kafka topic users, with the value_format of AVRO.

CREATE TABLE users \
  (registertime BIGINT, gender VARCHAR, regionid VARCHAR, userid VARCHAR PRIMARY KEY) \
  WITH (KAFKA_TOPIC='users', VALUE_FORMAT='AVRO');

Write Queries

Set the auto.offset.reset` query property to ``earliest. This instructs ksqlDB queries to read all available topic data from the beginning. This configuration is used for each subsequent query
SET 'auto.offset.reset'='earliest';

SET 'auto.offset.reset'='earliest';

Create a non-persistent query that returns data from a stream with the results limited to a maximum of three rows:
SELECT pageid FROM pageviews EMIT CHANGES LIMIT 3;

Create a persistent query (as a stream) that filters pageviews stream for female users. The results from this query are written to the Kafka PAGEVIEWS_FEMALE topic:
CREATE STREAM pageviews_female \
  AS SELECT users.userid AS userid, pageid, regionid, gender \
  FROM pageviews LEFT JOIN users ON pageviews.userid = users.userid \
  WHERE gender = 'FEMALE';


Create a persistent query where the regionid ends with 8 or 9. Results from this query are written to a Kafka topic named pageviews_enriched_r8_r9:
CREATE STREAM pageviews_female_like_89 \
  WITH (kafka_topic='pageviews_enriched_r8_r9', value_format='AVRO') \
  AS SELECT * FROM pageviews_female \
    WHERE regionid LIKE '%_8' OR regionid LIKE '%_9';

Create a persistent query that counts the pageviews for each region and gender combination in a tumbling window of 30 seconds when the count is greater than 1. Because the procedure is grouping and counting, the result is now a table, rather than a stream. Results from this query are written to a Kafka topic called PAGEVIEWS_REGIONS:

CREATE TABLE pageviews_regions \
  AS SELECT gender, regionid , COUNT(*) AS numusers \
  FROM pageviews LEFT JOIN users ON pageviews.userid = users.userid \
  WINDOW TUMBLING (size 30 second) \
  GROUP BY gender, regionid \
  HAVING COUNT(*) > 1;

SHOW STREAMS;
SHOW TABLES;
SHOW QUERIES;



Monitor Streaming Data
The following query returns the page view information of female users:
SELECT * FROM pageviews_female EMIT CHANGES;

The following query returns the page view information of female users in the regions whose regionid ends with 8 or 9:
SELECT * FROM pageviews_female_like_89 EMIT CHANGES;

The following query returns the page view counts for each region and gender combination in a tumbling window of 30 seconds.
SELECT * FROM pageviews_regions EMIT CHANGES;
