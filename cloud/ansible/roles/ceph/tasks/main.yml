---
# http://prashplus.blogspot.com/2018/01/ceph-single-node-setup-ubuntu.html
# tasks file for ceph

- name: "Install CEPH server"
  debug:
    msg: "Install CEPH server"

#- name: "Add CEPH GPG key"
#  apt_key:
#    url: "https://ceph.com/git/?p=ceph.git;a=blob_plain;f=keys/release.asc"
    #validate_certs: false

#- name: "Add CEPH APT repository"
#  apt_repository:
#    repo: "deb http://ceph.com/archive/debian-dumpling/ wheezy main"

- name: Install CEPH
  package:
    name: "{{item}}"
  with_items:
    - ceph-deploy
    - ceph-mds
    - ceph-common
    - dnsmasq


# http://prashplus.blogspot.com/2018/01/ceph-single-node-setup-ubuntu.html
# sudo useradd -m -s /bin/bash ceph-deploy
# sudo passwd ceph-deploy
- name: "Add ceph-deploy user"
  user:
    name: ceph-deploy
    shell: /bin/bash
    #groups: ceph-deploy
    password: lol

# sudo echo "ceph-deploy ALL = (root) NOPASSWD:ALL" | sudo tee /etc/sudoers.d/ceph-deploy
# sudo chmod 0440 /etc/sudoers.d/ceph-deploy
- name: "Add ceph-deploy as sudoer"
  lineinfile:
    path: /etc/sudoers.d/ceph-deploy
    line: "ceph-deploy ALL = (root) NOPASSWD:ALL"
    create: yes
    mode: 0440

# sudo su - ceph-deploy

# /etc/ssh/sshd_config - PasswordAuthentication yes - service ssh restart
- name: "Change ssh config"
  lineinfile:
    path: /etc/ssh/sshd_config
    regexp: 'PasswordAuthentication'
    line: "PasswordAuthentication yes"
  register: ssh_config

- name: Restart ssh service
  service:
    name: ssh
    state: restarted
  when: ssh_config.changed

# /etc/hosts - 192.168.99.50   ubuntu-data - 192.168.33.50   ubuntu-data
- name: "Update /etc/hosts"
  lineinfile:
    path: /etc/hosts
    line: "{{ item }}"
  with_items:
    - "192.168.99.50   ubuntu-data"
    - "192.168.33.50   ubuntu-data"

# mkdir /home/ceph-deploy/my-cluster
- name: "Create directories"
  file:
    path: "{{ item }}"
    state: directory
    mode: 0755
    owner: ceph-deploy
    group: ceph-deploy
  with_items:
    - "/home/ceph-deploy/my-cluster"
    - "/home/ceph-deploy/.ssh"
    - "/mnt/cephfs"

# /home/ceph-deploy/.ssh/config
- name: "add ssh config"
  template:
    src: ssh-config.tpl
    dest: /home/ceph-deploy/.ssh/config
    owner: ceph-deploy
    group: ceph-deploy

# ssh-keygen
- name: "ssh-keygen"
  command: ssh-keygen -q -t rsa -f /home/ceph-deploy/.ssh/id_rsa  -C '' -N ''
  args:
    creates: /home/ceph-deploy/.ssh/id_rsa.pub
  become_user: ceph-deploy

# ssh-copy-id ceph-deploy@ubuntu-data
- name: "Set authorized key taken from file"
  shell: touch /home/ceph-deploy/.ssh/authorized_keys && cat /home/ceph-deploy/.ssh/id_rsa.pub > /home/ceph-deploy/.ssh/authorized_keys
  args:
    creates: /home/ceph-deploy/.ssh/authorized_keys
  become_user: ceph-deploy

# ceph-deploy new ubuntu-data
- name: "deploy new cluster"
  shell: cd /home/ceph-deploy/my-cluster && ceph-deploy new ubuntu-data
  args:
    creates: /home/ceph-deploy/my-cluster/ceph.mon.keyring
  become_user: ceph-deploy

# ceph.conf - osd pool default size = 2 - osd crush chooseleaf type = 0
- name: "ceph.conf"
  lineinfile:
    path: /home/ceph-deploy/my-cluster/ceph.conf
    line: "{{ item }}"
  with_items:
    - "osd pool default size = 2"
    - "osd crush chooseleaf type = 0"

# ceph-deploy install ubuntu-data
- name: "ceph-deploy install ubuntu-data"
  shell: cd /home/ceph-deploy/my-cluster && ceph-deploy install ubuntu-data && touch /home/ceph-deploy/my-cluster/install-ubuntu-data.log
  args:
    creates: /home/ceph-deploy/my-cluster/install-ubuntu-data.log
  become_user: ceph-deploy

# ceph-deploy mon create-initial
- name: "ceph-deploy mon create-initial"
  shell: cd /home/ceph-deploy/my-cluster && ceph-deploy mon create-initial && touch /home/ceph-deploy/my-cluster/mon-create-initial.log
  args:
    creates: /home/ceph-deploy/my-cluster/mon-create-initial.log
  become_user: ceph-deploy

# ceph-deploy admin ubuntu-data
- name: "ceph-deploy admin ubuntu-data"
  shell: cd /home/ceph-deploy/my-cluster && ceph-deploy admin ubuntu-data && touch /home/ceph-deploy/my-cluster/admin-ubuntu-data.log
  args:
    creates: /home/ceph-deploy/my-cluster/admin-ubuntu-data.log
  become_user: ceph-deploy

# ceph-deploy mgr create ubuntu-data
- name: "ceph-deploy mgr create ubuntu-data"
  shell: cd /home/ceph-deploy/my-cluster && ceph-deploy mgr create ubuntu-data && touch /home/ceph-deploy/my-cluster/mgr-create-ubuntu-data.log
  args:
    creates: /home/ceph-deploy/my-cluster/mgr-create-ubuntu-data.log
  become_user: ceph-deploy

#  ceph-deploy mds create ubuntu-data
- name: "ceph-deploy mds create ubuntu-data"
  shell: cd /home/ceph-deploy/my-cluster && ceph-deploy mds create ubuntu-data && touch /home/ceph-deploy/my-cluster/mds-create-ubuntu-data.log
  args:
    creates: /home/ceph-deploy/my-cluster/mds-create-ubuntu-data.log
  become_user: ceph-deploy

# sudo ceph-disk zap /dev/sdc
# sudo ceph-disk zap /dev/sdd
- name: "ceph-disk zap"
  shell: "ceph-disk zap /dev/{{ item }} && touch /home/ceph-deploy/my-cluster/ceph-disk-zap-{{ item }}.log"
  args:
    creates: "/home/ceph-deploy/my-cluster/ceph-disk-zap-{{ item }}.log"
  with_items:
    "{{ ceph_disks }}"

# ceph-deploy osd prepare ubuntu-data:sdc
# ceph-deploy osd prepare ubuntu-data:sdd
- name: "ceph-deploy osd prepare"
  shell: "cd /home/ceph-deploy/my-cluster && ceph-deploy osd prepare ubuntu-data:{{ item }} && touch /home/ceph-deploy/my-cluster/osd-prepare-{{ item }}.log"
  args:
    creates: /home/ceph-deploy/my-cluster/osd-prepare-{{ item }}.log
  with_items:
    "{{ ceph_disks }}"
  ignore_errors: yes
  #no_log: True

# ceph-deploy osd activate ubuntu-data:/dev/sdc1
# ceph-deploy osd activate ubuntu-data:/dev/sdd1
- name: "ceph-deploy osd activate"
  shell: "cd /home/ceph-deploy/my-cluster && ceph-deploy osd activate ubuntu-data:/dev/{{ item }}1 && touch /home/ceph-deploy/my-cluster/osd-activate-{{ item }}.log"
  args:
    creates: /home/ceph-deploy/my-cluster/osd-activate-{{ item }}.log
  with_items:
    "{{ ceph_disks }}"
  ignore_errors: yes
  #no_log: True

# ceph-deploy osd create ubuntu-data:/dev/sdc
# ceph-deploy osd create ubuntu-data:/dev/sdd

- name: "Check CEPH health"
  shell: "ceph -s | grep health | tr -d ' ' | sed -e 's/health://g'"
  register: ceph_health

- debug:
    msg: "{{ ceph_health.stdout }}"

- name: "Check CEPH usage"
  shell: "ceph -s | grep usage | tr -d '' | sed -e 's/usage://g'"
  register: ceph_usage

- debug:
    msg: "{{ ceph_usage.stdout }}"

- name: "get ceph key"
  shell: "cat /home/ceph-deploy/my-cluster/ceph.client.admin.keyring | grep key | tr -d '\t' | sed -e 's/key = //g'"
  register: ceph_keyring

- debug:
    msg: "{{ ceph_keyring.stdout }}"

# CEPH FS

# sudo ceph osd pool create cephfs_data 128
# sudo ceph osd pool create cephfs_metadata 128
# sudo ceph fs new cephfs cephfs_metadata cephfs_data

#- name: "CEPH FS"
#  shell: |
    #ceph osd pool create cephfs_data 128
    #sudo ceph osd pool create cephfs_metadata 128
    #sudo ceph fs new cephfs cephfs_metadata cephfs_data

#    sudo ceph osd pool create kube 1024
#    sudo ceph auth get-or-create client.kube mon 'allow r, allow command "osd blacklist"' osd 'allow class-read object_prefix rbd_children, allow rwx pool=kube' -o ceph.client.kube.keyring

#    cd my-cluster && sudo ceph osd pool create kube 128 128
#    cd my-cluster && sudo ceph auth add client.kube mon 'allow r' osd 'allow rwx pool=kube'

#    touch /home/ceph-deploy/my-cluster/ceph-client.log
#  args:
#    creates: /home/ceph-deploy/my-cluster/ceph-client.log

# ceph auth get-or-create client.cephfs_data mon 'allow r, allow command "osd blacklist"' osd 'allow class-read object_prefix rbd_children, allow rwx pool=cephfs_data' -o ceph.client.cephfs_data.keyring

# sudo mkdir /mnt/cephfs

# cat /home/ceph-deploy/my-cluster/ceph.client.admin.keyring

- name: "Add admin.secret"
  lineinfile:
    path: /home/ceph-deploy/my-cluster/admin.secret
    line: "{{ ceph_keyring.stdout }}"
    create: yes

# sudo mount -t ceph ubuntu-data:6789:/ /mnt/cephfs -o name=admin,secret=AQAN0zlcQ4iTDxAARN7wnD8mxmyzRHsdxhgilQ==
# sudo mount -t ceph ubuntu-data:6789:/ /mnt/cephfs -o name=admin,secretfile=/home/ceph-deploy/my-cluster/admin.secret

#- name: "Mount CEPH fs"
#  mount:
#    path: /mnt/cephfs
#    src: "ubuntu-data:6789:/"
#    fstype: ceph
#    opts: "name=admin,secret={{ ceph_keyring.stdout }}"
#    state: present
